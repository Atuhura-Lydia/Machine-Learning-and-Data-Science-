# -*- coding: utf-8 -*-
"""Cervical cancer app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1n03f3LRz8kNpCZBzihB_cAe25_J4U9U9

# **PREDICTING RISK OF CERVICAL CANCER USING MACHINE LEARNING**

 ***Group 4 ; Adokorach Clare Rhonah , Atuheire Elizabeth, Atuhura Lydia***

Lydia:* Data loading, exploration,initial preprocessing and presentation preparation*

Clare: *KNN imputation, EDA visualizations, feature importance*

Elizabeth: *Model implementation, evaluation, Gradio (for demo)*

Elizabeth AND Clare: *Analysis, interpretation*
"""

# all necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.impute import KNNImputer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report, roc_curve
import warnings

warnings.filterwarnings('ignore')

# visualizations
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

# Load and explore the dataset  done by Lydia
from google.colab import drive
drive.mount('/content/gdrive')
df = pd.read_csv('/content/gdrive/MyDrive/risk_factors_cervical_cancer.csv')
print("Dataset Shape:", df.shape)
print("\nFirst few rows:")
print(df.head())

print("\nDataset Info:")
print(df.info())

print("\nMissing values summary:")
# Count missing values (represented as '?')
missing_count = (df == '?').sum()
print(missing_count[missing_count > 0])

print("\nTarget variable distribution (Biopsy):")
print(df['Biopsy'].value_counts())

# Data Preprocessing and Cleaning done by lydia

# Replace '?' with NaN for proper handling
df.replace('?', np.nan, inplace=True)

# Convert all columns to numeric where possible
for col in df.columns:
    df[col] = pd.to_numeric(df[col], errors='coerce')

print("Data types after conversion:")
print(df.dtypes.value_counts())

# Analyze missing values after conversion
print("\nMissing values percentage:")
missing_percentage = (df.isnull().sum() / len(df)) * 100
missing_percentage = missing_percentage[missing_percentage > 0].sort_values(ascending=False)
print(missing_percentage)

# Drop columns with too many missing values (>50%)
columns_to_drop = missing_percentage[missing_percentage > 50].index
print(f"\nDropping columns with >50% missing values: {list(columns_to_drop)}")
df_clean = df.drop(columns=columns_to_drop)

print(f"Shape after dropping high-missing columns: {df_clean.shape}")

# Handle remaining missing values using KNN Imputation done by clare

X = df_clean.drop('Biopsy', axis=1)
y = df_clean['Biopsy']

print("Before imputation - Missing values in features:")
print(X.isnull().sum().sum())

# KNN Imputer
imputer = KNNImputer(n_neighbors=5)
X_imputed = imputer.fit_transform(X)
X_imputed = pd.DataFrame(X_imputed, columns=X.columns)

print("After imputation - Missing values in features:")
print(X_imputed.isnull().sum().sum())

print(f"Final dataset shape: {X_imputed.shape}")
print(f"Target distribution: {y.value_counts()}")
print(f"Class ratio: {y.value_counts(normalize=True)}")

#EDA
# summary dataframe for analysis
analysis_df = X_imputed.copy()
analysis_df['Biopsy'] = y

# 1. Target distribution
plt.figure(figsize=(15, 12))

plt.subplot(2, 3, 1)
y.value_counts().plot(kind='bar', color=['skyblue', 'salmon'])
plt.title('Distribution of Biopsy Results')
plt.xlabel('Biopsy Result (0=Negative, 1=Positive)')
plt.ylabel('Count')
for i, v in enumerate(y.value_counts()):
    plt.text(i, v + 5, str(v), ha='center', va='bottom')

# 2. Age distribution by biopsy result ()
plt.subplot(2, 3, 2)
sns.boxplot(x='Biopsy', y='Age', data=analysis_df, palette=['skyblue', 'salmon'])
plt.title('Age Distribution by Biopsy Result')

# 3. Number of sexual partners by biopsy result
plt.subplot(2, 3, 3)
sns.boxplot(x='Biopsy', y='Number of sexual partners', data=analysis_df, palette=['skyblue', 'salmon'])
plt.title('Number of Sexual Partners by Biopsy Result')

# 4. Correlation heatmap
plt.subplot(2, 3, 4)
# Select top features for correlation
top_features = ['Age', 'Number of sexual partners', 'First sexual intercourse',
               'Num of pregnancies', 'Smokes', 'Hormonal Contraceptives',
               'STDs', 'STDs (number)', 'Biopsy']
corr_matrix = analysis_df[top_features].corr()
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0)
plt.title('Correlation Heatmap of Key Features')

# 5. STDs presence by biopsy result
plt.subplot(2, 3, 5)
std_biopsy = pd.crosstab(analysis_df['STDs'], analysis_df['Biopsy'])
std_biopsy.plot(kind='bar', color=['skyblue', 'salmon'])
plt.title('STDs Presence vs Biopsy Result')
plt.xlabel('STDs (0=No, 1=Yes)')

plt.tight_layout()
plt.show()

# 6. preview using Random Forest
plt.figure(figsize=(10, 8))
rf_preview = RandomForestClassifier(n_estimators=100, random_state=42)
rf_preview.fit(X_imputed, y)
feature_importance = pd.DataFrame({
    'feature': X_imputed.columns,
    'importance': rf_preview.feature_importances_
}).sort_values('importance', ascending=False)

plt.figure(figsize=(12, 8))
sns.barplot(data=feature_importance.head(15), x='importance', y='feature')
plt.title('Top 15 Most Important Features (Random Forest)')
plt.xlabel('Feature Importance')
plt.tight_layout()
plt.show()

#univariate
#histogram showing age distribution
ax=df['Age'].hist(bins='auto' ,edgecolor = 'black')
ax.ticklabel_format(style='plain');
ax.set_title('Distribution of Age')
ax.set_xlabel('Age (Years)')
ax.set_ylabel('Frequency')

# Multi variate analysis
 #Count value frequencies for each column
cols = ["Schiller", "Citology", "Biopsy"]
counts = pd.DataFrame()
for c in cols:
    counts[c] = df[c].value_counts()
# Transpose so columns become groups
counts = counts.T
# Plot grouped bar chart
counts.plot(kind="bar", figsize=(8,5))
plt.title("Counts of Target Variables")
plt.xlabel("Target Variable")
plt.ylabel("Count")
plt.legend(title="Value (0/1)")
plt.show()

# Prepare data for modeling done by Elizabeth

# Split the data
X_train, X_test, y_train, y_test = train_test_split(
    X_imputed, y, test_size=0.2, random_state=42, stratify=y
)

print(f"Training set size: {X_train.shape}")
print(f"Test set size: {X_test.shape}")
print(f"Training set class distribution:\n{y_train.value_counts()}")
print(f"Test set class distribution:\n{y_test.value_counts()}")

# Scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Convert to DataFrames for better handling
X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_imputed.columns)
X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_imputed.columns)

# Model Training and Evaluation done by Elizabeth

# Initialize models
models = {
    'Random Forest': RandomForestClassifier(random_state=42),
    'SVM': SVC(random_state=42, probability=True),
    'XGBoost': XGBClassifier(random_state=42, eval_metric='logloss')
}


results = {}

# Train and evaluate each model
for name, model in models.items():
    print(f"\n{'='*50}")
    print(f"Training {name}...")

    # Train model
    if name in ['RandomForestClassifier', 'SVM']:
        model.fit(X_train_scaled, y_train)
        y_pred = model.predict(X_test_scaled)
        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]
    else:
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        y_pred_proba = model.predict_proba(X_test)[:, 1]

    # Calculate metrics
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    auc_roc = roc_auc_score(y_test, y_pred_proba)

    # Store results
    results[name] = {
        'model': model,
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'auc_roc': auc_roc,
        'predictions': y_pred,
        'probabilities': y_pred_proba
    }

    print(f"{name} Results:")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1-Score: {f1:.4f}")
    print(f"AUC-ROC: {auc_roc:.4f}")

    # Classification report
    print(f"\nClassification Report for {name}:")
    print(classification_report(y_test, y_pred))

#neural network
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report
import warnings
warnings.filterwarnings('ignore')

print("\n==================================================")
print("Training MLPClassifier (Neural Network)...")

# Initialize MLPClassifier

mlp_model = MLPClassifier(
    hidden_layer_sizes=(100, 50),
    max_iter=1000,
    random_state=42,
    activation='relu',
    solver='adam', # optimizer
    learning_rate_init=0.001,
    alpha=0.0001, # L2 penalty (regularization term) parameter
    verbose=True
)

# Train the MLP model on the scaled training data
mlp_model.fit(X_train_scaled, y_train)

# Make predictions on the scaled test data
y_pred_mlp = mlp_model.predict(X_test_scaled)
y_pred_proba_mlp = mlp_model.predict_proba(X_test_scaled)[:, 1]

# Calculate metrics
accuracy_mlp = accuracy_score(y_test, y_pred_mlp)
precision_mlp = precision_score(y_test, y_pred_mlp)
recall_mlp = recall_score(y_test, y_pred_mlp)
f1_mlp = f1_score(y_test, y_pred_mlp)
auc_roc_mlp = roc_auc_score(y_test, y_pred_proba_mlp)

# Store results for comparison
results['MLP (Neural Network)'] = {
    'model': mlp_model,
    'accuracy': accuracy_mlp,
    'precision': precision_mlp,
    'recall': recall_mlp,
    'f1': f1_mlp,
    'auc_roc': auc_roc_mlp,
    'predictions': y_pred_mlp,
    'probabilities': y_pred_proba_mlp
}

print("\nMLP (Neural Network) Results:")
print(f"Accuracy: {accuracy_mlp:.4f}")
print(f"Precision: {precision_mlp:.4f}")
print(f"Recall: {recall_mlp:.4f}")
print(f"F1-Score: {f1_mlp:.4f}")
print(f"AUC-ROC: {auc_roc_mlp:.4f}")

# Classification report
print(f"\nClassification Report for MLP (Neural Network):")
print(classification_report(y_test, y_pred_mlp))

# Model Comparison and Visualization by Elizabeth and Clare

# Comparison DataFrame
results_df = pd.DataFrame({
    name: [
        results[name]['accuracy'],
        results[name]['precision'],
        results[name]['recall'],
        results[name]['f1'],
        results[name]['auc_roc']
    ] for name in results.keys()
}, index=['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC-ROC']).T

print("Model Performance Comparison:")
print(results_df.round(4))

# Plot performance comparison
plt.figure(figsize=(15, 10))

# Metrics comparison
plt.subplot(2, 2, 1)
results_df[['Accuracy', 'Precision', 'Recall', 'F1-Score']].plot(kind='bar', ax=plt.gca())
plt.title('Model Performance Metrics Comparison')
plt.xticks(rotation=45)
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')

# AUC-ROC comparison
plt.subplot(2, 2, 2)
results_df['AUC-ROC'].plot(kind='bar', color='pink', ax=plt.gca())
plt.title('AUC-ROC Score Comparison')
plt.xticks(rotation=45)
plt.ylabel('AUC-ROC Score')

# ROC Curves
plt.subplot(2, 2, 3)
for name in models.keys():
    fpr, tpr, _ = roc_curve(y_test, results[name]['probabilities'])
    plt.plot(fpr, tpr, label=f'{name} (AUC = {results[name]["auc_roc"]:.3f})')

plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curves')
plt.legend()

# Focus on Recall (most important metric for medical diagnosis)
plt.subplot(2, 2, 4)
results_df['Recall'].plot(kind='bar', color='red', ax=plt.gca())
plt.title('Recall Score Comparison (Minimize False Negatives)')
plt.xticks(rotation=45)
plt.ylabel('Recall Score')

plt.tight_layout()
plt.show()

#Cross-Validation for more robust evaluation by Elizabeth
print("Performing Cross-Validation...")

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
cv_results = {}

for name, model in models.items():
    print(f"\nCross-validating {name}...")

    if name in ['Logistic Regression', 'SVM']:
        X_cv = X_train_scaled
    else:
        X_cv = X_train

    # Cross-validation scores
    cv_accuracy = cross_val_score(model, X_cv, y_train, cv=cv, scoring='accuracy')
    cv_precision = cross_val_score(model, X_cv, y_train, cv=cv, scoring='precision')
    cv_recall = cross_val_score(model, X_cv, y_train, cv=cv, scoring='recall')
    cv_f1 = cross_val_score(model, X_cv, y_train, cv=cv, scoring='f1')
    cv_auc = cross_val_score(model, X_cv, y_train, cv=cv, scoring='roc_auc')

    cv_results[name] = {
        'CV Accuracy': cv_accuracy.mean(),
        'CV Precision': cv_precision.mean(),
        'CV Recall': cv_recall.mean(),
        'CV F1': cv_f1.mean(),
        'CV AUC': cv_auc.mean()
    }

    print(f"CV Accuracy: {cv_accuracy.mean():.4f} (+/- {cv_accuracy.std() * 2:.4f})")
    print(f"CV Recall: {cv_recall.mean():.4f} (+/- {cv_recall.std() * 2:.4f})")

# CV comparison
cv_df = pd.DataFrame(cv_results).T
print("\nCross-Validation Results:")
print(cv_df.round(4))

import joblib
# Hyperparameter Tuning for the best model by Lydia and Clare

# tune the best performing model
print("Performing Hyperparameter Tuning...")

# Let's assume Random Forest or XGBoost performed best
best_model_name = max(results.keys(), key=lambda x: results[x]['recall'])
print(f"Tuning the best model: {best_model_name}")

if best_model_name == 'Random Forest':
    param_grid = {
        'n_estimators': [100, 200, 300],
        'max_depth': [10, 20, None],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4]
    }
    model = RandomForestClassifier(random_state=42)

elif best_model_name == 'XGBoost':
    param_grid = {
        'n_estimators': [100, 200, 300],
        'max_depth': [3, 6, 9],
        'learning_rate': [0.01, 0.1, 0.2],
        'subsample': [0.8, 0.9, 1.0]
    }
    model = XGBClassifier(random_state=42, eval_metric='logloss')

elif best_model_name == 'Logistic Regression':
    param_grid = {
        'C': [0.1, 1, 10, 100],
        'penalty': ['l1', 'l2'],
        'solver': ['liblinear', 'saga']
    }
    model = LogisticRegression(random_state=42, max_iter=1000)

else:
    param_grid = {
        'C': [0.1, 1, 10, 100],
        'gamma': ['scale', 'auto', 0.1, 0.01],
        'kernel': ['rbf', 'linear']
    }
    model = SVC(random_state=42, probability=True)

# Perform grid search with recall as scoring metric
grid_search = GridSearchCV(
    model, param_grid, cv=5, scoring='recall',
    n_jobs=-1, verbose=1
)

if best_model_name in ['Logistic Regression', 'SVM']:
    grid_search.fit(X_train_scaled, y_train)
else:
    grid_search.fit(X_train, y_train)

print(f"Best parameters: {grid_search.best_params_}")
print(f"Best cross-validation recall: {grid_search.best_score_:.4f}")

# Train final model with best parameters
best_model = grid_search.best_estimator_

# Save the best model, scaler, and feature names
joblib.dump(best_model, 'best_model.pkl')
joblib.dump(scaler, 'scaler.pkl')
joblib.dump(X_imputed.columns.tolist(), 'feature_names.pkl')
print("Best model, scaler, and feature names saved.")

if best_model_name in ['Logistic Regression', 'SVM']:
    y_pred_final = best_model.predict(X_test_scaled)
    y_pred_proba_final = best_model.predict_proba(X_test_scaled)[:, 1]
else:
    y_pred_final = best_model.predict(X_test)
    y_pred_proba_final = best_model.predict_proba(X_test)[:, 1]

# Evaluate final tuned model
final_accuracy = accuracy_score(y_test, y_pred_final)
final_precision = precision_score(y_test, y_pred_final)
final_recall = recall_score(y_test, y_pred_final)
final_f1 = f1_score(y_test, y_pred_final)
final_auc = roc_auc_score(y_test, y_pred_proba_final)

print(f"\nFinal Tuned Model Performance:")
print(f"Accuracy: {final_accuracy:.4f}")
print(f"Precision: {final_precision:.4f}")
print(f"Recall: {final_recall:.4f}")
print(f"F1-Score: {final_f1:.4f}")
print(f"AUC-ROC: {final_auc:.4f}")

# Feature Importance Analysis and Model Interpretation by lydia

if hasattr(best_model, 'feature_importances_'):
    plt.figure(figsize=(12, 8))

    feature_importance = pd.DataFrame({
        'feature': X_imputed.columns,
        'importance': best_model.feature_importances_
    }).sort_values('importance', ascending=False)

    # Plot top 15 features
    sns.barplot(data=feature_importance.head(15), x='importance', y='feature')
    plt.title(f'Top 15 Most Important Features - {best_model_name}')
    plt.xlabel('Feature Importance')
    plt.tight_layout()
    plt.show()

    print("Top 10 Most Important Features:")
    print(feature_importance.head(10))

elif best_model_name == 'Logistic Regression':
    # For logistic regression, we can look at coefficients
    plt.figure(figsize=(12, 8))

    coefficients = pd.DataFrame({
        'feature': X_imputed.columns,
        'coefficient': best_model.coef_[0]
    }).sort_values('coefficient', key=abs, ascending=False)

    # Plot top 10 coefficients
    sns.barplot(data=coefficients.head(10), x='coefficient', y='feature')
    plt.title(f'Top 15 Most Influential Features - {best_model_name}')
    plt.xlabel('Coefficient Value')
    plt.tight_layout()
    plt.show()

    print("Top 10 Most Influential Features:")
    print(coefficients.head(10))

# deployment-ready prediction function

def predict_cervical_cancer_risk(patient_data, model=best_model, scaler=scaler, feature_names=X_imputed.columns):
    """
    Predict cervical cancer risk for a new patient

    Parameters:
    patient_data: dict or array-like containing patient features
    model: trained model
    scaler: fitted scaler
    feature_names: list of feature names

    Returns:
    dict: prediction results
    """

    # Convert to DataFrame if it's a dictionary
    if isinstance(patient_data, dict):
        patient_df = pd.DataFrame([patient_data])
        # Ensure all features are present and in correct order
        for feature in feature_names:
            if feature not in patient_df.columns:
                patient_df[feature] = 0
        patient_df = patient_df[feature_names]
    else:
        patient_df = pd.DataFrame([patient_data], columns=feature_names)

    # Scale the data if needed
    if hasattr(model, 'feature_importances_'):
        prediction = model.predict(patient_df)[0]
        probability = model.predict_proba(patient_df)[0, 1]
    else:
        patient_scaled = scaler.transform(patient_df)
        # Convert scaled array back to DataFrame with feature names to avoid UserWarning
        patient_scaled_df = pd.DataFrame(patient_scaled, columns=feature_names)
        prediction = model.predict(patient_scaled_df)[0]
        probability = model.predict_proba(patient_scaled_df)[:, 1][0]

    risk_level = "High Risk" if prediction == 1 else "Low Risk"
    confidence = probability if prediction == 1 else (1 - probability)

    return {
        'prediction': prediction,
        'probability': probability,
        'risk_level': risk_level,
        'confidence': confidence
    }

# Example usage:
print("Example Prediction:")
sample_patient = {
    'Age': 35,
    'Number of sexual partners': 3,
    'First sexual intercourse': 17,
    'Num of pregnancies': 2,
    'Smokes': 0,
    'Hormonal Contraceptives': 1,
    'IUD': 0,
    'STDs': 0,
    'STDs (number)': 0
}

# Add remaining features with default values
for feature in X_imputed.columns:
    if feature not in sample_patient:
        sample_patient[feature] = 0

prediction_result = predict_cervical_cancer_risk(sample_patient)
print(f"Prediction: {prediction_result}")

# gradio_app.py
import gradio as gr
import pandas as pd
import numpy as np
import joblib
from sklearn.preprocessing import StandardScaler
import os

# Load model function
def load_model():
    """Load the trained model and scaler"""
    try:
        model = joblib.load('best_model.pkl')
        scaler = joblib.load('scaler.pkl')
        feature_names = joblib.load('feature_names.pkl')

        print(f"DEBUG: Loaded model type: {type(model)}")
        print(f"DEBUG: Loaded scaler type: {type(scaler)}")
        print(f"DEBUG: Loaded feature_names type: {type(feature_names)}")

        # Explicit check that loaded objects are not None
        if model is None:
            print("Warning: Model loaded as None.")
            return None, None, None, False
        if scaler is None:
            print("Warning: Scaler loaded as None.")
            return None, None, None, False
        if feature_names is None:
            print("Warning: Feature names loaded as None.")
            return None, None, None, False

        print("Model components loaded successfully. (Inside load_model)")
        return model, scaler, feature_names, True
    except FileNotFoundError as e:
        print(f"Model files not found: {e}. Please run train_model.py first. (Inside load_model)")
        return None, None, None, False
    except Exception as e:
        print(f"Error loading model components: {e} (Inside load_model)")
        return None, None, None, False

# Load model at startup
model, scaler, feature_names, model_loaded = load_model()

def predict_risk(age, num_partners, first_sex, pregnancies, smokes,
                hormonal_contraceptives, iud, stds, stds_number,
                smokes_years=0, hormonal_years=0, iud_years=0, stds_condylomatosis=0):
    """
    Predict cervical cancer risk based on patient inputs
    """
    print(f"predict_risk called: model_loaded={model_loaded}")
    if not model_loaded:
        print("predict_risk: Model not loaded, returning error message.")
        return "‚ùå Model not loaded. Please train the model first.", 0.5, "Unknown"

    try:
        # Debugging prints
        print(f"predict_risk: model type={type(model)}, scaler type={type(scaler)}, feature_names present={feature_names is not None}")
        print(f"predict_risk: hasattr(model, 'feature_importances_') = {hasattr(model, 'feature_importances_')}")

        # Prepare input data
        input_data = {
            'Age': age,
            'Number of sexual partners': num_partners,
            'First sexual intercourse': first_sex,
            'Num of pregnancies': pregnancies,
            'Smokes': 1 if smokes else 0,
            'Hormonal Contraceptives': 1 if hormonal_contraceptives else 0,
            'IUD': 1 if iud else 0,
            'STDs': 1 if stds else 0,
            'STDs (number)': stds_number,
            'Smokes (years)': smokes_years,
            'Hormonal Contraceptives (years)': hormonal_years,
            'IUD (years)': iud_years,
            'STDs:condylomatosis': 1 if stds_condylomatosis else 0
        }

        # Fill all features expected by the model
        full_data = {}
        # Ensure feature_names is not None before iterating
        if feature_names is None:
             raise ValueError("Feature names are not loaded, cannot prepare input data.")

        for feature in feature_names:
            full_data[feature] = input_data.get(feature, 0)

        # Convert to DataFrame
        patient_df = pd.DataFrame([full_data])
        patient_df = patient_df[feature_names]

        # Scale and predict
        if hasattr(model, 'feature_importances_'):
            print("predict_risk: Model is tree-based, skipping scaler.transform.")
            prediction = model.predict(patient_df)[0]
            probability = model.predict_proba(patient_df)[0, 1]
        else:
            print("predict_risk: Model requires scaling.")
            # This branch is for models that require scaling (like SVM, Logistic Regression, MLP).
            if scaler is None:
                 raise ValueError("Scaler object is None, but the loaded model requires data scaling. Please ensure scaler.pkl is valid.")
            patient_scaled = scaler.transform(patient_df)
            # Convert scaled array back to DataFrame with feature names to avoid UserWarning
            patient_scaled_df = pd.DataFrame(patient_scaled, columns=feature_names)
            prediction = model.predict(patient_scaled_df)[0]
            probability = model.predict_proba(patient_scaled_df)[:, 1][0]

        # Determine risk level and recommendations
        if prediction == 1:
            risk_level = "üö® HIGH RISK"
            recommendations = """
            **Immediate Actions Required:**
            ‚Ä¢ Schedule biopsy within 2 weeks
            ‚Ä¢ Refer for colposcopy examination
            ‚Ä¢ Conduct HPV DNA testing
            ‚Ä¢ Close monitoring required
            """
        else:
            risk_level = "‚úÖ LOW RISK"
            recommendations = """
            **Routine Care Plan:**
            ‚Ä¢ Continue routine screening schedule
            ‚Ä¢ Annual Pap smear recommended
            ‚Ä¢ Regular follow-up appointments
            ‚Ä¢ Maintain healthy lifestyle
            """

        # Risk factors analysis
        risk_factors = []
        if age < 20 or age > 50:
            risk_factors.append(f"Age ({age})")
        if num_partners > 4:
            risk_factors.append(f"Multiple sexual partners ({num_partners})")
        if first_sex < 16:
            risk_factors.append(f"Early sexual debut (age {first_sex})")
        if stds:
            risk_factors.append("History of STDs")
        if smokes:
            risk_factors.append("Smoking")

        risk_factors_text = "‚Ä¢ " + "\\n‚Ä¢ ".join(risk_factors) if risk_factors else "‚Ä¢ No significant risk factors identified"

        result_text = f"""
## üìä Assessment Results

**Risk Level:** {risk_level}
**Probability:** {probability:.1%}

### üîç Identified Risk Factors:
{risk_factors_text}

### üìã Clinical Recommendations:
{recommendations}

**Model Confidence:** {max(probability, 1-probability):.1%}
"""

        return result_text, probability, "High Risk" if prediction == 1 else "Low Risk"

    except Exception as e:
        print(f"predict_risk: Exception caught: {e}")
        return f"‚ùå Prediction error: {str(e)}", 0.5, "Error"

# Create Gradio interface
with gr.Blocks(theme=gr.themes.Soft(), title="Cervical Cancer Risk Assessment") as demo:
    gr.Markdown("# ü©∫ Cervical Cancer Risk Assessment Tool")
    gr.Markdown("**A machine learning-based clinical decision support system**")

    with gr.Row():
        with gr.Column():
            gr.Markdown("### Patient Demographics")
            age = gr.Slider(minimum=15, maximum=80, value=35, label="Age")
            num_partners = gr.Slider(minimum=1, maximum=30, value=3, label="Number of sexual partners")
            first_sex = gr.Slider(minimum=10, maximum=30, value=17, label="Age at first sexual intercourse")
            pregnancies = gr.Slider(minimum=0, maximum=15, value=2, label="Number of pregnancies")

        with gr.Column():
            gr.Markdown("### Lifestyle & Medical History")
            smokes = gr.Checkbox(label="Current smoker")
            hormonal_contraceptives = gr.Checkbox(label="Uses hormonal contraceptives")
            iud = gr.Checkbox(label="Uses IUD")
            stds = gr.Checkbox(label="History of STDs")
            stds_number = gr.Slider(minimum=0, maximum=10, value=0, label="Number of STD diagnoses")

    with gr.Accordion("Additional Medical History", open=False):
        with gr.Row():
            smokes_years = gr.Slider(minimum=0, maximum=40, value=0, label="Years smoking")
            hormonal_years = gr.Slider(minimum=0, maximum=30, value=0, label="Years using hormonal contraceptives")
            iud_years = gr.Slider(minimum=0, maximum=20, value=0, label="Years using IUD")
            stds_condylomatosis = gr.Checkbox(label="History of condylomatosis")

    predict_btn = gr.Button("üîç Assess Cancer Risk", variant="primary")

    with gr.Row():
        with gr.Column():
            risk_output = gr.Markdown(label="Assessment Results")
        with gr.Column():
            probability_gauge = gr.Label(label="Risk Probability")
            risk_level = gr.Label(label="Risk Level")

    # Model status
    if model_loaded:
        gr.Markdown(f"‚úÖ **Model Status:** Loaded successfully ({len(feature_names)} features)")
    else:
        gr.Markdown("‚ùå **Model Status:** Not loaded. Please run `python train_model.py` first.")

    # Examples
    gr.Markdown("### üí° Example Cases")
    examples = [
        [35, 3, 17, 2, False, True, False, False, 0, 0, 0, 0, False],  # Low risk
        [45, 8, 15, 5, True, False, False, True, 2, 15, 0, 0, True],   # High risk
        [28, 2, 19, 1, False, True, False, False, 0, 0, 3, 0, False]   # Low risk
    ]

    # Footer
    gr.Markdown("---")
    gr.Markdown("""
    **Note:** This tool is for clinical decision support demonstration only.
    Always consult with qualified healthcare professionals for medical advice.

    *Built with your trained machine learning model*
    """)

    # Connect interface
    predict_btn.click(
        fn=predict_risk,
        inputs=[age, num_partners, first_sex, pregnancies, smokes,
                hormonal_contraceptives, iud, stds, stds_number,
                smokes_years, hormonal_years, iud_years, stds_condylomatosis],
        outputs=[risk_output, probability_gauge, risk_level]
    )

if __name__ == "__main__":
    demo.launch(share=True)

"""### 1. Check Git Installation and Install Git LFS

First, check if Git is installed. If not, you might need to install it. Then, install Git LFS.
"""

# Check Git version (optional)
!git --version

# Install Git LFS
!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash
!sudo apt-get install git-lfs
!git lfs install

"""### 2. Initialize a Git Repository (if you haven't already)

Navigate to your project directory. For this example, let's assume your model files are in `/content/` (where they were saved).
"""

# Change directory to where your model files are located
# If they are in /content/, you might not need to change directory.
# For example, if you saved them in a subfolder in Google Drive:
# !cd /content/gdrive/MyDrive/your_project_folder

!git init
!git config user.email "you@example.com"
!git config user.name "Your Name"

"""### 3. Track Model Files with Git LFS

Tell Git LFS to track all `.pkl` files. This means that instead of storing the actual large file in the Git repository, Git will store a pointer to the file, and the actual file content will be stored on the Git LFS server.
"""

!git lfs track "*.pkl"

"""### 4. Add Files and Commit

Now, add your files to the staging area and make your commit. Notice that `.gitattributes` is also added, which stores the LFS tracking configuration.
"""

!git add .gitattributes best_model.pkl scaler.pkl feature_names.pkl
!git add gradio_app.py # Add your Gradio app script
# !git add . # Or add all current files

!git commit -m "Add Gradio app and track model files with Git LFS"

"""### 5. Add Remote and Push

Finally, add your remote repository (e.g., GitHub) and push your changes. The large `.pkl` files will be uploaded to Git LFS.
"""

# Replace with your actual GitHub repository URL
!git remote add origin https://github.com/your-username/your-repo-name.git

# Push to GitHub
!git push -u origin main # or 'master'